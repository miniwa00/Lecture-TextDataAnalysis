{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934fe100",
   "metadata": {},
   "source": [
    "### [KOR]\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>4. </b>NLTK에서 제공하는 tweet데이터를 csv형태로 변환한 파일들이 'tweets'폴더 안에 존재한다. 이 중, tweets.20150430-223406.tweet.csv 안의 text column들의 내용을 (할 수 있는 만큼만) 전처리하시오.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>NLTK패키지에서 제공하는 json파일을 csv로 변환하는 방법 참고 사이트</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>https://blog.devgenius.io/getting-started-with-nltk-twitter-corpus-b9bb5b757fbd</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b353fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d055cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user.id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Apr 30 21:34:06 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>593891099434983425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @KirkKus: Indirect cost of the UK being in ...</td>\n",
       "      <td>False</td>\n",
       "      <td>107794703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Apr 30 21:34:06 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>593891099548094465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>VIDEO: Sturgeon on post-election deals http://...</td>\n",
       "      <td>False</td>\n",
       "      <td>557422508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Apr 30 21:34:06 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>593891099388846080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @LabourEoin: The economy was growing 3 time...</td>\n",
       "      <td>False</td>\n",
       "      <td>3006692193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Apr 30 21:34:06 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>593891100429045760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @GregLauder: the UKIP east lothian candidat...</td>\n",
       "      <td>False</td>\n",
       "      <td>455154030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Apr 30 21:34:07 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>593891100768784384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @thesundaypeople: UKIP's housing spokesman ...</td>\n",
       "      <td>False</td>\n",
       "      <td>187547338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  favorite_count                  id  \\\n",
       "0  Thu Apr 30 21:34:06 +0000 2015               0  593891099434983425   \n",
       "1  Thu Apr 30 21:34:06 +0000 2015               0  593891099548094465   \n",
       "2  Thu Apr 30 21:34:06 +0000 2015               0  593891099388846080   \n",
       "3  Thu Apr 30 21:34:06 +0000 2015               0  593891100429045760   \n",
       "4  Thu Apr 30 21:34:07 +0000 2015               0  593891100768784384   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_user_id  retweet_count  retweeted  \\\n",
       "0                    NaN                  NaN              0      False   \n",
       "1                    NaN                  NaN              0      False   \n",
       "2                    NaN                  NaN              0      False   \n",
       "3                    NaN                  NaN              0      False   \n",
       "4                    NaN                  NaN              0      False   \n",
       "\n",
       "                                                text  truncated     user.id  \n",
       "0  RT @KirkKus: Indirect cost of the UK being in ...      False   107794703  \n",
       "1  VIDEO: Sturgeon on post-election deals http://...      False   557422508  \n",
       "2  RT @LabourEoin: The economy was growing 3 time...      False  3006692193  \n",
       "3  RT @GregLauder: the UKIP east lothian candidat...      False   455154030  \n",
       "4  RT @thesundaypeople: UKIP's housing spokesman ...      False   187547338  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweets/tweets.20150430-223406.tweet.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fd019f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        RT @KirkKus: Indirect cost of the UK being in ...\n",
       "1        VIDEO: Sturgeon on post-election deals http://...\n",
       "2        RT @LabourEoin: The economy was growing 3 time...\n",
       "3        RT @GregLauder: the UKIP east lothian candidat...\n",
       "4        RT @thesundaypeople: UKIP's housing spokesman ...\n",
       "                               ...                        \n",
       "19995    RT @UKLabour: .@Ed_Miliband: we're not going t...\n",
       "19996    RT @DisabledScot: @blairmcdougall @ScotlandTon...\n",
       "19997    RT @Staircase2: @VividRicky exactly but that a...\n",
       "19998    Actually agreed with %95 of what farage was sa...\n",
       "19999    RT @PeatWorrier: \"A vote for the SNP is. Er. A...\n",
       "Name: text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2923cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = tweets['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615b92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = []\n",
    "patterns = [r'http\\S+|www\\S+', r'RT\\s+@\\S+\\s?:\\s', r'#\\w+\\s?', r\"\\'\", r'\\\"', r'\\.\\@\\w+', r'@\\w+: ', r'^@\\w+\\s+', r'[^\\w\\s]']\n",
    "for text in text_list:\n",
    "    text = text.replace('\\n','').replace('&amp','')\n",
    "    while True:\n",
    "        if text[0] == '@':\n",
    "            text = re.sub(r'@\\S+', '', text)\n",
    "        break\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    list_1.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c97431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = []\n",
    "pattern = r'^@\\w+\\s+'\n",
    "for text in list_1:\n",
    "    list_2.append(re.sub(pattern, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbb050b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_3 = []\n",
    "pattern = r'^@\\w+\\s+'\n",
    "for text in list_2:\n",
    "    list_3.append(re.sub(pattern, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e9c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_4 = []\n",
    "pattern = r'^@\\w+\\s+'\n",
    "for text in list_3:\n",
    "    list_4.append(re.sub(pattern, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c61c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_5 = []\n",
    "pattern = r'^@\\w+\\s+'\n",
    "for text in list_4:\n",
    "    list_5.append(re.sub(pattern, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b65ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_6 = []\n",
    "pattern = r'^@\\w+\\s+'\n",
    "for text in list_5:\n",
    "    text.replace('^ +', \"\")\n",
    "    text = re.sub(pattern, '', text.strip())\n",
    "    text = re.sub(r':\\s*', '', text)\n",
    "    list_6.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a006faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [s for s in list_6 if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ca1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "\n",
    "results = []\n",
    "for text in texts:\n",
    "    sent_tokens = nltk.sent_tokenize(text)\n",
    "    for sent in sent_tokens:\n",
    "        sent = sent.lower() # 문장을 소문자로 변환\n",
    "        result = tokenizer.tokenize(sent) # 각 문장을 단어 토큰화\n",
    "        results.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28b52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.tag.pos_tag(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a72d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "173fd9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "converted_tags = [(tag[0], convert_tag(tag[1])) for tag in tags]\n",
    "lemmas = []\n",
    "for tag in converted_tags:\n",
    "    if tag[1] is None:\n",
    "        lemmas.append(lemmatizer.lemmatize(tag[0]))\n",
    "    else:\n",
    "        lemmas.append(lemmatizer.lemmatize(tag[0], tag[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ccdd6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어 불용어 리스트\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어가 아닌 단어만 추출하여 리스트로 저장\n",
    "all_words = [word for word in lemmas if not word in stop_words]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
